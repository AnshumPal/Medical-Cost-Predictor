{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMLIaU/NXBqJa5gymP2dAjp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SbYKSWZ9EQlK"},"outputs":[],"source":["# importing the data from the website\n","import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"mirichoi0218/insurance\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","source":["#reading the data\n","import pandas as pd\n","\n","df = pd.read_csv(\"insurance.csv\")\n","print(df.head())\n","print(df.shape)"],"metadata":{"id":"wqyu3PcLF4EI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking if there are any null value in the dataset\n","df.isnull()"],"metadata":{"id":"pUn3jWr3G3zs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"WGg2o5eFHCkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# it will drop all the rows which contain atleast 1 missing value\n","df.dropna(axis=0)\n","\n","# it will drop all the coloumns which contain atleast 1 missing value\n","df.dropna(axis=1)"],"metadata":{"id":"0t5m85_uHRXL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from  sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","\n","# converting the data\n","df['age'] = df['age'].astype('float64')\n","df['bmi'] = df['bmi'].astype('float64')\n","df['children'] = df['children'].astype('int64')\n","df['charges'] = df['charges'].astype('float64')\n","\n","\n","# handling the Categorical variable (One-Hot-Coding)\n","categorical_columns = ['sex', 'smoker', 'region']\n","df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n","\n","\n","# We are scaling the numeric columns (age, bmi, children, charges) to have a mean of 0 and a standard deviation of 1, making the data more suitable for machine learning models.\n","Scaler = StandardScaler()\n","numeric_columns = ['age', 'bmi', 'children']\n","df[numeric_columns] = Scaler.fit_transform(df[numeric_columns])\n","\n","\n","# assuming charge is the target value and dropping it\n","X = df.drop('charges', axis=1)\n","y = df['charges']\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n","\n"],"metadata":{"id":"0dTd5QglHjtb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Model training\n"," from sklearn.ensemble import RandomForestRegressor\n","\n"," model = RandomForestRegressor(random_state=56)\n"," model.fit(X_train, y_train)"],"metadata":{"id":"nyE9FfaBIXs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model evaluation\n","\n","\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","y_pred = model.predict(X_test)\n","\n","\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r2)"],"metadata":{"id":"a723oKYANNZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameter Tuning with GridSearchCV for the better performance of the model which try different combination and train the model for the better performance\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Split data into X and y\n","X = df.drop('charges', axis=1)\n","y = df['charges']\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the preprocessor (standard scaling for numerical data, one-hot encoding for categorical)\n","numerical_columns = ['age', 'bmi', 'children']\n","categorical_columns = ['sex_male', 'smoker_yes', 'region_southwest', 'region_northwest', 'region_southeast']\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numerical_columns),\n","        ('cat', OneHotEncoder(drop='first'), categorical_columns)\n","    ]\n",")\n","\n","# Define the model inside a pipeline\n","model = Pipeline(steps=[\n","    ('preprocessor', preprocessor),\n","    ('regressor', RandomForestRegressor(random_state=42))\n","])\n","\n","# Hyperparameter grid for tuning\n","param_grid = {\n","    'regressor__n_estimators': [100, 200, 300],\n","    'regressor__max_depth': [10, 20, None],\n","    'regressor__min_samples_split': [2, 5, 10],\n","    'regressor__min_samples_leaf': [1, 2, 4]\n","}\n","\n","# GridSearchCV for hyperparameter tuning\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n","\n","# Fit the grid search to the training data\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best model and score\n","print(\"Best Hyperparameters:\", grid_search.best_params_)\n","print(\"Best Score:\", grid_search.best_score_)\n","\n","# Evaluate the best model\n","best_model = grid_search.best_estimator_\n","test_score = best_model.score(X_test, y_test)\n","print(\"Test Score of Best Model:\", test_score)\n"],"metadata":{"id":"4I-m8d1VNiwA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cross validation :divide the data into small chunks and train the data and test it and then again repeat the process testing with a different chunks and training with other\n","from sklearn.model_selection import cross_val_score\n","\n","# Initialize the model\n","rf = RandomForestRegressor(random_state=42)\n","\n","# Perform cross-validation\n","cv_scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\n","\n","# Mean cross-validation score\n","print(\"Cross-validation MSE scores:\", -cv_scores)\n","print(\"Mean CV MSE:\", -cv_scores.mean())\n"],"metadata":{"id":"C8j_ptdkO09J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving the model for future use\n","import joblib\n","\n","# Assuming you've run GridSearchCV and stored the best model in `grid_search`\n","best_rf = grid_search.best_estimator_\n","\n","# Save the model\n","joblib.dump(best_rf, 'best_random_forest_model.pkl')\n","\n","# Load the model\n","loaded_model = joblib.load('best_random_forest_model.pkl')\n","\n","# Use the model to make predictions\n","loaded_predictions = loaded_model.predict(X_test)\n"],"metadata":{"id":"SFnsuxVYQ0jI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import mean_squared_error, r2_score\n","import numpy as np\n","\n","# 1. **Data Overview**: Getting a quick glance at the data\n","print(\"Data Overview:\")\n","print(df.head())  # View the first few rows\n","\n","# 2. **Summary Statistics**: Checking summary statistics of numeric columns\n","print(\"\\nSummary Statistics:\")\n","print(df.describe())  # Statistical summary\n","\n","# 3. **Data Visualizations**:\n","\n","## a. **Histograms**: To visualize the distribution of numeric columns\n","df[['age', 'bmi', 'children', 'charges']].hist(bins=15, figsize=(12, 8))\n","plt.suptitle('Histograms of Numeric Columns', y=1.02)\n","plt.show()\n","\n","## b. **Pair Plot**: To visualize the relationships between features\n","sns.pairplot(df[['age', 'bmi', 'children', 'charges']])\n","plt.suptitle('Pair Plot of Numeric Features', y=1.02)\n","plt.show()\n","\n","## c. **Correlation Heatmap**: To check the correlation between features\n","corr_matrix = df[['age', 'bmi', 'children', 'charges']].corr()\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n","plt.title('Correlation Heatmap')\n","plt.show()\n","\n","# 4. **Model Performance Visualizations**:\n","\n","## a. **Regression Metrics**: Evaluate model performance using regression metrics\n","y_pred = best_rf.predict(X_test)\n","\n","# Mean Squared Error and R-squared\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f\"Mean Squared Error: {mse}\")\n","print(f\"R-squared: {r2}\")\n","\n","# Extract the trained RandomForest model from the pipeline\n","model = best_rf.named_steps['regressor']  # Changed from 'randomforestclassifier' to 'regressor'\n","\n","# Now, get the feature importances\n","importances = model.feature_importances_\n","# Now, get the feature importances\n","importances = model.feature_importances_\n","features = X_train.columns\n","\n","# Sort features by importance\n","indices = np.argsort(importances)[::-1]\n","\n","# Plot Feature Importances\n","plt.figure(figsize=(12, 8))\n","sns.barplot(x=importances[indices], y=features[indices], palette='viridis')\n","plt.title('Feature Importances')\n","plt.show()\n","\n","# Saving the Model (optional)\n","import joblib\n","joblib.dump(best_rf, 'best_random_forest_model.pkl')  # Save the model\n","\n"],"metadata":{"id":"n6_7jD-CRDGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EQwVU5XSSP1n"},"execution_count":null,"outputs":[]}]}